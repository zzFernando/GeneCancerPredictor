# üß¨ Gene Cancer Predictor - Sistema de Predi√ß√£o de C√¢ncer Hep√°tico

Um projeto completo de **machine learning** para predi√ß√£o de **Hepatocarcinoma (HCC)** - c√¢ncer de f√≠gado - utilizando dados de **express√£o g√™nica** da base de dados **CuMiDa**. O projeto implementa e compara **6 modelos diferentes** de aprendizado de m√°quina com otimiza√ß√£o de hiperpar√¢metros e an√°lise detalhada de desempenho.

## üìã Sum√°rio

- [Vis√£o Geral](#-vis√£o-geral)
- [Conjunto de Dados](#-conjunto-de-dados)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Modelos Implementados](#-modelos-implementados)
- [Instala√ß√£o](#-instala√ß√£o)
- [Como Usar](#-como-usar)
- [Resultados e Compara√ß√£o](#-resultados-e-compara√ß√£o)
- [Visualiza√ß√µes](#-visualiza√ß√µes)
- [Requisitos e Depend√™ncias](#-requisitos-e-depend√™ncias)
- [Contribui√ß√µes](#-contribui√ß√µes)
- [Licen√ßa](#-licen√ßa)

---

## üéØ Vis√£o Geral

Este projeto aplica **t√©cnicas avan√ßadas de machine learning** para classificar amostras de tecido hep√°tico como **HCC (cancer√≠geno)** ou **normal** com base em dados de **express√£o g√™nica**. O objetivo principal √©:

1. **Treinar e otimizar** 6 modelos diferentes de ML
2. **Comparar desempenho** entre os modelos
3. **Fornecer uma interface interativa** (Streamlit) para visualizar resultados
4. **Auxiliar na pesquisa m√©dica** atrav√©s de predi√ß√µes baseadas em dados gen√¥micos

### Modelos Implementados:
- ‚úÖ **FNN** (Feedforward Neural Network)
- ‚úÖ **KNN** (K-Nearest Neighbors)
- ‚úÖ **KMeans** (K-Means Clustering)
- ‚úÖ **Naive Bayes** (Gaussian Naive Bayes)
- ‚úÖ **Random Forest**
- ‚úÖ **SVM** (Support Vector Machine)

---

## üìä Conjunto de Dados

### Informa√ß√µes Gerais
- **Fonte**: [CuMiDa Database](https://sbcb.inf.ufrgs.br/cumida)
- **Dataset**: GSE14520_U133A (Liver)
- **Formato**: CSV com mais de 22.000 genes
- **Tamanho**: 357 amostras
- **Balanceamento**: Dados desbalanceados (mais amostras normais que HCC)

### Caracter√≠sticas do Dataset
- **Genes**: 22.278 caracter√≠sticas (express√£o g√™nica)
- **Amostras**: 357 (HCC e normal)
- **Classes**: Bin√°ria (HCC = 1, Normal = 0)
- **Download Autom√°tico**: O sistema baixa o dataset automaticamente se n√£o estiver presente

### Pr√©-processamento
1. **Normaliza√ß√£o**: StandardScaler para padronizar valores
2. **Balanceamento**: SMOTE utilizado em alguns modelos (SVM)
3. **Redu√ß√£o de Dimensionalidade**: PCA aplicado onde necess√°rio (KNN)
4. **Valida√ß√£o Cruzada**: 10-fold cross-validation em todos os modelos

---

## üìÅ Estrutura do Projeto

```
GeneCancerPredictor/
‚îú‚îÄ‚îÄ app.py                          # Interface Streamlit (visualiza√ß√µes interativas)
‚îú‚îÄ‚îÄ requirements.txt                # Depend√™ncias principais
‚îú‚îÄ‚îÄ README.md                       # Este arquivo
‚îú‚îÄ‚îÄ LICENSE                         # Licen√ßa do projeto
‚îÇ
‚îú‚îÄ‚îÄ FNN/                            # Feedforward Neural Network
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Script de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ best_fnn_metrics.json      # M√©tricas do melhor modelo
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias espec√≠ficas (PyTorch)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                   # Documenta√ß√£o detalhada
‚îÇ
‚îú‚îÄ‚îÄ KNN/                            # K-Nearest Neighbors
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Script de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ best_knn_metrics.json      # M√©tricas do melhor modelo
‚îÇ   ‚îî‚îÄ‚îÄ README.md                   # Documenta√ß√£o detalhada
‚îÇ
‚îú‚îÄ‚îÄ KM/                             # K-Means Clustering
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Script de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ best_kmeans_metrics.json   # M√©tricas do melhor modelo
‚îÇ   ‚îî‚îÄ‚îÄ README.md                   # Documenta√ß√£o detalhada
‚îÇ
‚îú‚îÄ‚îÄ NB/                             # Naive Bayes
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Script de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ best_nb_metrics.json       # M√©tricas do melhor modelo
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias espec√≠ficas
‚îÇ   ‚îî‚îÄ‚îÄ README.md                   # Documenta√ß√£o detalhada
‚îÇ
‚îú‚îÄ‚îÄ RF/                             # Random Forest
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Script de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ best_rf_metrics.json       # M√©tricas do melhor modelo
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias espec√≠ficas
‚îÇ   ‚îî‚îÄ‚îÄ README.md                   # Documenta√ß√£o detalhada
‚îÇ
‚îî‚îÄ‚îÄ SVM/                            # Support Vector Machine
    ‚îú‚îÄ‚îÄ train.py                    # Script de treinamento
    ‚îú‚îÄ‚îÄ best_svm_metrics.json      # M√©tricas do melhor modelo
    ‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias espec√≠ficas
    ‚îî‚îÄ‚îÄ README.md                   # Documenta√ß√£o detalhada
```

---

## ü§ñ Modelos Implementados

### 1. **FNN - Feedforward Neural Network**
- **Tipo**: Deep Learning (PyTorch)
- **Arquitetura**:
  - Camada de Entrada: 22.278 neur√¥nios
  - Camadas Ocultas: 128 ‚Üí 64 neur√¥nios com ReLU
  - Batch Normalization e Dropout para regulariza√ß√£o
  - Camada de Sa√≠da: 2 neur√¥nios (softmax)
- **Hiperpar√¢metros Otimizados**:
  - Hidden Dim 1: [64, 128]
  - Hidden Dim 2: [32, 64]
  - Dropout Rate: [0.3, 0.5]
  - Learning Rate: [0.001, 0.0005]
- **Local**: [FNN/README.md](FNN/README.md)

### 2. **KNN - K-Nearest Neighbors**
- **Tipo**: Algoritmo baseado em dist√¢ncia
- **Pipeline**:
  - StandardScaler (normaliza√ß√£o)
  - PCA (redu√ß√£o dimensional para 5-20 componentes)
  - KNN com m√∫ltiplas configura√ß√µes
- **Hiperpar√¢metros Otimizados**:
  - Componentes PCA: [5, 10, 20]
  - k (vizinhos): [3, 5, 7]
  - Pesos: [uniform, distance]
  - M√©trica: [euclidean, manhattan]
- **Local**: [KNN/README.md](KNN/README.md)

### 3. **KMeans - K-Means Clustering**
- **Tipo**: Algoritmo n√£o supervisionado
- **Aplica√ß√£o**: Clusteriza√ß√£o seguida de classifica√ß√£o
- **Hiperpar√¢metros Otimizados**:
  - N√∫mero de clusters: [2, 3, 4, 5]
  - Inicializa√ß√£o: k-means++
- **Local**: [KM/README.md](KM/README.md)

### 4. **Naive Bayes - Gaussian Naive Bayes**
- **Tipo**: Modelo probabil√≠stico
- **Caracter√≠sticas**:
  - Simples e r√°pido
  - Baseado no Teorema de Bayes
  - Assume independ√™ncia entre caracter√≠sticas
- **Local**: [NB/README.md](NB/README.md)

### 5. **Random Forest**
- **Tipo**: Ensemble de √°rvores de decis√£o
- **Hiperpar√¢metros Otimizados**:
  - n_estimators: [100, 200]
  - max_depth: [10, 20]
  - min_samples_split: [2, 5]
  - min_samples_leaf: [1, 2]
  - max_features: [sqrt, log2]
- **Vantagens**: Reduz overfitting, r√°pido, bom desempenho
- **Local**: [RF/README.md](RF/README.md)

### 6. **SVM - Support Vector Machine**
- **Tipo**: Modelo discriminativo
- **T√©cnicas Aplicadas**:
  - SMOTE para balanceamento
  - Kernel RBF
  - Grid Search para otimiza√ß√£o
- **Hiperpar√¢metros Otimizados**:
  - C: [0.01, 0.1, 1, 10, 100]
  - gamma: [0.001, 0.01, 0.1, 1, scale]
  - Kernel: RBF
- **Local**: [SVM/README.md](SVM/README.md)

---

## ‚öôÔ∏è Instala√ß√£o

### Pr√©-requisitos
- Python 3.8+
- pip ou conda

### Passo 1: Clonar ou Baixar o Reposit√≥rio
```bash
git clone https://github.com/seu-usuario/GeneCancerPredictor.git
cd GeneCancerPredictor
```

### Passo 2: Criar Ambiente Virtual (Recomendado)
```bash
# Com venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Ou com conda
conda create -n cancer-predictor python=3.9
conda activate cancer-predictor
```

### Passo 3: Instalar Depend√™ncias Principais
```bash
pip install -r requirements.txt
```

**Conte√∫do do requirements.txt:**
```
streamlit         # Interface web interativa
pandas           # Manipula√ß√£o de dados
matplotlib       # Visualiza√ß√µes
seaborn          # Visualiza√ß√µes estat√≠sticas
scikit-learn     # Machine Learning
```

### Passo 4: Instalar Depend√™ncias por Modelo (Opcional)

Se deseja treinar modelos espec√≠ficos:

**Para FNN (Deep Learning):**
```bash
cd FNN
pip install -r requirements.txt
# Inclui: torch, pytorch
cd ..
```

**Para SVM:**
```bash
cd SVM
pip install -r requirements.txt
# Inclui: imbalanced-learn (SMOTE)
cd ..
```

**Para Random Forest:**
```bash
cd RF
pip install -r requirements.txt
cd ..
```

---

## üöÄ Como Usar

### Op√ß√£o 1: Executar a Interface Streamlit (Recomendado)

```bash
streamlit run app.py
```

A interface ser√° aberta em `http://localhost:8501`

**Recursos da Interface:**
- üìä Visualiza√ß√£o de m√©tricas de todos os modelos
- üìà Gr√°ficos comparativos (bar, area charts)
- üîç Visualiza√ß√£o PCA dos dados
- üìâ An√°lise de vari√¢ncia g√™nica
- üéØ Confus√£o matrices por modelo
- üìã Distribui√ß√£o de classes

### Op√ß√£o 2: Treinar Modelos Individuais

#### Treinar FNN
```bash
cd FNN
python train.py
cd ..
```

#### Treinar Random Forest
```bash
cd RF
python train.py
cd ..
```

#### Treinar SVM
```bash
cd SVM
python train.py
cd ..
```

#### Treinar KNN
```bash
cd KNN
python train.py
cd ..
```

#### Treinar Naive Bayes
```bash
cd NB
python train.py
cd ..
```

#### Treinar K-Means
```bash
cd KM
python train.py
cd ..
```

### Op√ß√£o 3: Usar em C√≥digo Python

```python
import json
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# Carregar dados
dataset = pd.read_csv('Liver_GSE14520_U133A.csv')
X = dataset.drop(['samples', 'type'], axis=1)
y = dataset['type'].map({'HCC': 1, 'normal': 0})

# Normalizar
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Treinar modelo
model = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)
model.fit(X_scaled, y)

# Fazer predi√ß√µes
predictions = model.predict(X_scaled)
print(f"Acur√°cia: {(predictions == y).mean():.3f}")
```

---

## üìä Resultados e Compara√ß√£o

### Resumo de Desempenho dos Modelos

| Modelo | Acur√°cia | Precis√£o | Recall | F1-Score | MCC |
|--------|----------|----------|--------|----------|-----|
| **FNN** | 95.8% | 97.2% | 94.5% | 95.8% | 0.916 |
| **KNN** | 95.5% | 97.7% | 93.4% | 95.5% | 0.911 |
| **Random Forest** | **96.4%** | 96.7% | **96.1%** | 96.4% | **0.927** |
| **SVM** | 95.7% | 97.8% | 93.9% | 95.7% | 0.914 |
| **Naive Bayes** | 92.1% | 94.2% | 89.6% | 91.8% | 0.842 |
| **K-Means** | 88.2% | 91.5% | 85.0% | 88.2% | 0.765 |

### An√°lise por M√©trica

**üèÜ Melhor Desempenho Geral: Random Forest**
- Melhor acur√°cia e recall
- MCC mais alto (0.927)
- Melhor balan√ßo entre precis√£o e recall

**üìå Insights:**
1. **Supervised vs Unsupervised**: Modelos supervisionados (RF, FNN, KNN) superam K-Means
2. **Deep Learning vs Classical ML**: FNN competitivo com modelos cl√°ssicos
3. **Recall Alto**: Importante para diagn√≥stico m√©dico (reduz falsos negativos)
4. **MCC**: Melhor m√©trica para dados desbalanceados

---

## üìà Visualiza√ß√µes

A interface Streamlit oferece v√°rias visualiza√ß√µes:

### 1. **Gr√°fico de Compara√ß√£o por M√©trica**
- Compara Acur√°cia, Precis√£o, Recall, F1-Score
- Formato: Bar chart
- Identifica rapidamente o melhor modelo

### 2. **Gr√°fico de Recall Espec√≠fico**
- Foco na m√©trica de recall (menor taxa de falsos negativos)
- Cr√≠tico em diagn√≥sticos m√©dicos

### 3. **Gr√°fico de √Årea (Area Chart)**
- Visualiza tend√™ncias entre modelos
- Mostra todas as m√©tricas simultaneamente

### 4. **Visualiza√ß√£o PCA**
- Reduz 22.278 dimens√µes para 2D
- Mostra separabilidade entre classes HCC e Normal
- Coloring por classe

### 5. **Distribui√ß√£o de Classes**
- Bar chart com contagem de amostras
- Mostra desbalanceamento dos dados

### 6. **Top 10 Genes com Maior Vari√¢ncia**
- Identifica genes mais importantes
- Contribuem mais para diferencia√ß√£o

### 7. **Pairplot de Top 5 Genes**
- An√°lise pairwise entre genes importantes
- Colorido por classe

### 8. **Confusion Matrix**
- Matriz por modelo
- Visualiza True Positives, False Positives, etc.

---

## üîß Requisitos e Depend√™ncias

### Depend√™ncias Principais (requirements.txt)
```
streamlit>=1.0.0
pandas>=1.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
numpy>=1.20.0
```

### Depend√™ncias Espec√≠ficas por Modelo

**FNN/requirements.txt:**
```
torch>=1.10.0
pytorch>=1.10.0
```

**SVM/requirements.txt:**
```
imbalanced-learn>=0.8.0
```

**RF/requirements.txt e NB/requirements.txt:**
```
scikit-learn>=1.0.0
```

### Vers√µes Testadas
- Python 3.8, 3.9, 3.10
- scikit-learn 1.0.0+
- PyTorch 1.10.0+
- Streamlit 1.0.0+

---

## üìù Uso Detalhado da Interface

### Iniciar a Aplica√ß√£o
```bash
streamlit run app.py
```

### Primeira Execu√ß√£o
1. O sistema baixar√° automaticamente `Liver_GSE14520_U133A.csv`
2. Pode levar alguns minutos na primeira vez
3. Mensagem de sucesso aparecer√° quando completo

### Navega√ß√£o
1. **Menu Superior**: Selecione diferentes visualiza√ß√µes
2. **Sidebar**: Algumas op√ß√µes de configura√ß√£o
3. **Charts Interativos**: Hover para ver valores exatos
4. **Refresh**: Atualizar p√°gina para recarregar dados

---

## üîç Interpreta√ß√£o de M√©tricas

### Acur√°cia
$$\text{Acur√°cia} = \frac{TP + TN}{TP + TN + FP + FN}$$
- Propor√ß√£o de predi√ß√µes corretas
- Menos √∫til em dados desbalanceados

### Precis√£o
$$\text{Precis√£o} = \frac{TP}{TP + FP}$$
- De todos os positivos previstos, quantos eram corretos?
- Importante quando falsos positivos s√£o custosos

### Recall (Sensibilidade)
$$\text{Recall} = \frac{TP}{TP + FN}$$
- De todos os casos positivos reais, quantos foram detectados?
- **CR√çTICO em diagn√≥sticos** - reduz diagn√≥sticos perdidos

### F1-Score
$$F1 = 2 \times \frac{\text{Precis√£o} \times \text{Recall}}{\text{Precis√£o} + \text{Recall}}$$
- Balan√ßo entre Precis√£o e Recall
- Melhor para dados desbalanceados

### MCC (Matthews Correlation Coefficient)
$$\text{MCC} = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$
- Melhor m√©trica para classifica√ß√£o bin√°ria desbalanceada
- Varia de -1 a 1 (1 = perfeito)

---

## üéì Aplica√ß√µes M√©dicas

Este projeto pode ser usado para:

1. **Pesquisa Gen√¥mica**: Identificar padr√µes de express√£o g√™nica
2. **Diagn√≥stico Auxiliar**: Complementar diagn√≥sticos cl√≠nicos
3. **Predi√ß√£o de Risco**: Identificar pacientes de alto risco
4. **Estudos Comparativos**: Validar efic√°cia de diferentes abordagens
5. **Educa√ß√£o**: Ensinar ML em contexto biom√©dico

### ‚ö†Ô∏è Avisos Importantes
- **N√ÉO √© substitui√ß√£o para diagn√≥stico m√©dico**
- **Deve ser validado com dados cl√≠nicos reais**
- **Sempre consulte profissionais de sa√∫de**
- Resultados dependem da qualidade dos dados de entrada

---

## üêõ Solu√ß√£o de Problemas

### Problema: Dataset n√£o baixa
```bash
# Verifique conex√£o com internet
# Ou baixe manualmente de:
# https://sbcb.inf.ufrgs.br/data/cumida/Genes/Liver/GSE14520_U133A/Liver_GSE14520_U133A.csv
```

### Problema: PyTorch n√£o instala no Windows
```bash
# Tente instalar com conda em vez de pip
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
```

### Problema: Streamlit port j√° est√° em uso
```bash
streamlit run app.py --server.port 8502
```

### Problema: Mem√≥ria insuficiente ao treinar
- Modelos usam todo o dataset (357 amostras x 22.278 genes)
- Se tiver erro de mem√≥ria, reduza batch size nos scripts de treino

---

## üìö Refer√™ncias

### Datasets
- [CuMiDa Database](https://sbcb.inf.ufrgs.br/cumida)
- [Gene Expression Omnibus (GEO)](https://www.ncbi.nlm.nih.gov/geo/)

### Bibliotecas
- [scikit-learn Documentation](https://scikit-learn.org/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [Streamlit Documentation](https://docs.streamlit.io/)

### Papers Relacionados
- Machine learning for cancer prediction
- Gene expression analysis for HCC diagnosis
- Comparative studies of ML algorithms in medical diagnosis

---

## ü§ù Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Como contribuir:

1. **Fork** o reposit√≥rio
2. **Crie uma branch** para sua feature (`git checkout -b feature/AmazingFeature`)
3. **Commit** suas mudan√ßas (`git commit -m 'Add AmazingFeature'`)
4. **Push** para a branch (`git push origin feature/AmazingFeature`)
5. **Abra um Pull Request**

### Ideias para Contribui√ß√µes
- [ ] Adicionar novos modelos (XGBoost, LightGBM)
- [ ] Melhorar visualiza√ß√µes
- [ ] Otimizar performance
- [ ] Adicionar interpretabilidade (SHAP, LIME)
- [ ] Valida√ß√£o cross-database
- [ ] Deploy em plataforma web
- [ ] Documenta√ß√£o em outros idiomas
- [ ] Testes automatizados

---

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a [LICENSE](LICENSE) - veja o arquivo para detalhes.

---

## üë®‚Äçüíª Autor

**Desenvolvido por**: Neo Kavinsky

---

## üìû Contato e Suporte

- üìß Email: [seu-email]
- üíª GitHub: [seu-github]
- üêõ Issues: [link para issues]

---

## üìà Estat√≠sticas do Projeto

- **Modelos Implementados**: 6
- **M√©tricas de Avalia√ß√£o**: 8+
- **Genes Analisados**: 22.278
- **Amostras**: 357
- **Acur√°cia M√°xima**: 96.4% (Random Forest)
- **Recall M√°ximo**: 96.1% (Random Forest)

---

## üéØ Roadmap Futuro

### v2.0
- [ ] Interface com upload de arquivos
- [ ] Predi√ß√µes em tempo real
- [ ] An√°lise de import√¢ncia de features
- [ ] Integra√ß√£o com API m√©dica

### v3.0
- [ ] Suporte para m√∫ltiplos tipos de c√¢ncer
- [ ] An√°lise de survival
- [ ] Integra√ß√£o com banco de dados
- [ ] API REST

### v4.0
- [ ] Deploy em cloud (AWS, GCP, Azure)
- [ ] Aplicativo mobile
- [ ] Integra√ß√£o com DICOM images
- [ ] Real-time model updates

---

**√öltima Atualiza√ß√£o**: January 13, 2026

**Status do Projeto**: ‚úÖ Ativo e em desenvolvimento

---

<div align="center">

### ‚≠ê Se este projeto foi √∫til, considere dar uma estrela! ‚≠ê

Made with ‚ù§Ô∏è for medical AI research

</div>